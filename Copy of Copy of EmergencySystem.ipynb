{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Copy of Copy of EmergencySystem.ipynb","provenance":[{"file_id":"1CjbdmIKY04WJ3JDfjD_wxOoFdsrCYemY","timestamp":1588464642383},{"file_id":"1HeUXwmwFMbORnLNTjBfpRI9lGp-IlLAg","timestamp":1588456450397}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"rjYVcKL5dm5K","colab_type":"text"},"source":["# MOUNT The Drive"]},{"cell_type":"code","metadata":{"id":"HydPh7KQgD2n","colab_type":"code","outputId":"707404fa-fe8d-41c3-94a3-6fbc7ec40502","executionInfo":{"status":"ok","timestamp":1588432907874,"user_tz":-120,"elapsed":20226,"user":{"displayName":"MOHAMMAD KHALED MOSLEMANY","photoUrl":"","userId":"11179830557597909442"}},"colab":{"base_uri":"https://localhost:8080/","height":124}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"dNlqbqV7_z1o","colab_type":"text"},"source":["# Preprocessing"]},{"cell_type":"code","metadata":{"id":"qKUMmlCV_y7l","colab_type":"code","colab":{}},"source":["video_path = \"\"\n","\n","img_arr=[]\n","cap = cv2.VideoCapture(video_path)\n","success, img = cap.read()\n","idx = 0\n","num=1\n","while success:\n","  idx+=1\n","  if num==idx:\n","    img_arr.append(img)\n","    num+=6\n","  success, img = cap.read()\n","\n","height, width, layers = img_arr[0].shape\n","size = (width,height)\n","\n","out = cv2.VideoWriter('preprocess.avi',cv2.VideoWriter_fourcc(*'DIVX'), 30, size)\n","\n","for i in range(len(img_arr)):\n","    out.write(img_arr[i])\n","\n","out.release()\n","cv2.destroyAllWindows()  \n","\n","filename = \"/contetn/preprocess.avi\""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ElEEEiMUc_KX","colab_type":"code","colab":{}},"source":["def avg (amg_arr) :\n","  res = [] \n","  avg = []\n","  if len(img_arr) %20 ==0 :\n","    for i in img_arr :\n","      res.append(i) \n","      if len(res) == 20 :\n","        height, width, layers = res[0].shape\n","        size = (width,height)\n","        out = cv2.VideoWriter('preprocess.avi',cv2.VideoWriter_fourcc(*'DIVX'), 20 , size)\n","        for i in range(len(res)):\n","          out.write(res[i])\n","        out.release()\n","        cv2.destroyAllWindows()\n","        avg.append(violence_predict(out))\n","        res.clear() \n","  else : \n","    for i in range (img_arr) :\n","      if i == len(img_arr)-(len(img_arr%20)) :\n","        for j in range(img_arr[-20:]) :\n","          res.append(img_arr[j])\n","          if len(res) == 20 :\n","            height, width, layers = res[0].shape\n","            size = (width,height)\n","            out = cv2.VideoWriter('preprocess.avi',cv2.VideoWriter_fourcc(*'DIVX'), 20 , size)\n","            for i in range(len(res)):\n","              out.write(res[i])\n","            out.release()\n","            cv2.destroyAllWindows()\n","            avg.append(violence_predict(out))\n","            res.clear()\n","      else :\n","        res.append(img_arr[i])\n","        if len(res) == 20 :\n","          height, width, layers = res[0].shape\n","          size = (width,height)\n","          out = cv2.VideoWriter('preprocess.avi',cv2.VideoWriter_fourcc(*'DIVX'), 20 , size)\n","          for i in range(len(res)):\n","            out.write(res[i])\n","          out.release()\n","          cv2.destroyAllWindows()\n","          avg.append(violence_predict(out))\n","          res.clear()\n","  for i in avg :\n","    if i == 1 :\n","      return True \n","    else :\n","      return False \n","\n","\n","      \n","         \n","         \n","      \n","\n","\n","\n","\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jd8xEI5pl60z","colab_type":"text"},"source":["# Violence preparation\n"]},{"cell_type":"code","metadata":{"id":"G_JRqpDpn9No","colab_type":"code","colab":{}},"source":["\n","######################################\n","##################################\n","#\n","# Functions for downloading and extracting data-files from the internet.\n","#\n","# Implemented in Python 3.5\n","#\n","########################################################################\n","#\n","# This file is part of the TensorFlow Tutorials available at:\n","#\n","# https://github.com/Hvass-Labs/TensorFlow-Tutorials\n","#\n","# Published under the MIT License. See the file LICENSE for details.\n","#\n","# Copyright 2016 by Magnus Erik Hvass Pedersen\n","#\n","########################################################################\n","\n","import sys\n","import os\n","import urllib.request\n","import tarfile\n","import zipfile\n","\n","########################################################################\n","\n","\n","def _print_download_progress(count, block_size, total_size):\n","    \"\"\"\n","    Function used for printing the download progress.\n","    Used as a call-back function in maybe_download_and_extract().\n","    \"\"\"\n","\n","    # Percentage completion.\n","    pct_complete = float(count * block_size) / total_size\n","\n","    # Status-message. Note the \\r which means the line should overwrite itself.\n","    msg = \"\\r- Download progress: {0:.1%}\".format(pct_complete)\n","\n","    # Print it.\n","    sys.stdout.write(msg)\n","    sys.stdout.flush()\n","\n","\n","########################################################################\n","\n","\n","def maybe_download_and_extract(url, download_dir):\n","    \"\"\"\n","    Download and extract the data if it doesn't already exist.\n","    Assumes the url is a tar-ball file.\n","    :param url:\n","        Internet URL for the tar-file to download.\n","        Example: \"https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\"\n","    :param download_dir:\n","        Directory where the downloaded file is saved.\n","        Example: \"data/CIFAR-10/\"\n","    :return:\n","        Nothing.\n","    \"\"\"\n","\n","    # Filename for saving the file downloaded from the internet.\n","    # Use the filename from the URL and add it to the download_dir.\n","    filename = url.split('/')[-1]\n","    file_path = os.path.join(download_dir, filename)\n","\n","    # Check if the file already exists.\n","    # If it exists then we assume it has also been extracted,\n","    # otherwise we need to download and extract it now.\n","    if not os.path.exists(file_path):\n","        # Check if the download directory exists, otherwise create it.\n","        if not os.path.exists(download_dir):\n","            os.makedirs(download_dir)\n","\n","        # Download the file from the internet.\n","        file_path, _ = urllib.request.urlretrieve(url=url,\n","                                                  filename=file_path,\n","                                                  reporthook=_print_download_progress)\n","\n","        print()\n","        print(\"Download finished. Extracting files.\")\n","\n","        if file_path.endswith(\".zip\"):\n","            # Unpack the zip-file.\n","            zipfile.ZipFile(file=file_path, mode=\"r\").extractall(download_dir)\n","        elif file_path.endswith((\".tar.gz\", \".tgz\")):\n","            # Unpack the tar-ball.\n","            tarfile.open(name=file_path, mode=\"r:gz\").extractall(download_dir)\n","\n","        print(\"Done.\")\n","    else:\n","        print(\"Data has apparently already been downloaded and unpacked.\")\n","\n","\n","########################################################################\n","\n","\n","\n","%matplotlib inline\n","import cv2\n","import os\n","import numpy as np\n","import keras\n","import matplotlib.pyplot as plt\n","#import download\n","from random import shuffle\n","from keras.applications import VGG16\n","from keras import backend as K\n","from keras.models import Model, Sequential\n","from keras.layers import Input\n","from keras.layers import LSTM\n","from keras.layers import Dense, Activation\n","import sys\n","import h5py\n","\n","\n","def print_progress(count, max_count):\n","    # Percentage completion.\n","    pct_complete = count / max_count\n","\n","    # Status-message. Note the \\r which means the line should\n","    # overwrite itself.\n","    msg = \"\\r- Progress: {0:.1%}\".format(pct_complete)\n","\n","    # Print it.\n","    sys.stdout.write(msg)\n","    sys.stdout.flush()\n","\n","\n","\n","\n","def download_data(in_dir, url):\n","    \n","    if not os.path.exists(in_dir):\n","        os.makedirs(in_dir)\n","    \n","    maybe_download_and_extract(url,in_dir)\n","\n","\n","\n","\n","\n","in_dir = \"data\"\n","\n","url_hockey = \"http://visilab.etsii.uclm.es/personas/oscar/FightDetection/HockeyFights.zip\"\n","\n","download_data(in_dir,url_hockey)\n","\n","# Frame size  \n","img_size = 224\n","\n","img_size_touple = (img_size, img_size)\n","\n","# Number of channels (RGB)\n","num_channels = 3\n","\n","# Flat frame size\n","img_size_flat = img_size * img_size * num_channels\n","\n","# Number of classes for classification (Violence-No Violence)\n","num_classes = 2\n","\n","# Number of files to train\n","_num_files_train = 1\n","\n","# Number of frames per video\n","_images_per_file = 20\n","\n","# Number of frames per training set\n","_num_images_train = _num_files_train * _images_per_file\n","\n","# Video extension\n","video_exts = \".avi\"\n","\n","\n","def get_frames(current_dir, file_name):\n","    \n","    in_file = os.path.join(current_dir, file_name)\n","    \n","    images = []\n","    \n","    vidcap = cv2.VideoCapture(in_file)\n","    \n","    success,image = vidcap.read()\n","        \n","    count = 0\n","\n","    while count<_images_per_file:\n","                \n","        RGB_img = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","    \n","        res = cv2.resize(RGB_img, dsize=(img_size, img_size),\n","                                 interpolation=cv2.INTER_CUBIC)\n","    \n","        images.append(res)\n","    \n","        success,image = vidcap.read()\n","    \n","        count += 1\n","        \n","    resul = np.array(images)\n","    \n","    resul = (resul / 255.).astype(np.float16)\n","        \n","    return resul\n","\n","\n","\n","def label_video_names(in_dir):\n","    \n","    # list containing video names\n","    names = []\n","    # list containin video labels [1, 0] if it has violence and [0, 1] if not\n","    labels = []\n","    \n","    \n","    for current_dir, dir_names,file_names in os.walk(in_dir):\n","        \n","        for file_name in file_names:\n","            \n","            if file_name[0:2] == 'fi':\n","                labels.append([1,0])\n","                names.append(file_name)\n","            elif file_name[0:2] == 'no':\n","                labels.append([0,1])\n","                names.append(file_name)\n","                     \n","            \n","    c = list(zip(names,labels))\n","    # Suffle the data (names and labels)\n","    shuffle(c)\n","    \n","    names, labels = zip(*c)\n","            \n","    return names, labels\n","\n","\n","# First get the names and labels of the whole videos\n","names, labels = label_video_names(in_dir)\n","\n","image_model = VGG16(include_top=True, weights='imagenet')\n","\n","input_shape = image_model.layers[0].output_shape[1:3]\n","\n","\n","\n","# We will use the output of the layer prior to the final\n","# classification-layer which is named fc2. This is a fully-connected (or dense) layer.\n","transfer_layer = image_model.get_layer('fc2')\n","\n","image_model_transfer = Model(inputs=image_model.input,\n","                             outputs=transfer_layer.output)\n","\n","transfer_values_size = K.int_shape(transfer_layer.output)[1]\n","\n","\n","print(\"The input of the VGG16 net have dimensions:\",K.int_shape(image_model.input)[1:3])\n","\n","print(\"The output of the selecter layer of VGG16 net have dimensions: \", transfer_values_size)\n","\n","\n","\n","\n","\n","def get_transfer_values(current_dir, file_name):\n","    \n","    # Pre-allocate input-batch-array for images.\n","    shape = (_images_per_file,) + img_size_touple + (3,)\n","    \n","    image_batch = np.zeros(shape=shape, dtype=np.float16)\n","    \n","    image_batch = get_frames(current_dir, file_name)\n","      \n","    # Pre-allocate output-array for transfer-values.\n","    # Note that we use 16-bit floating-points to save memory.\n","    shape = (_images_per_file, transfer_values_size)\n","    transfer_values = np.zeros(shape=shape, dtype=np.float16)\n","\n","    transfer_values = \\\n","            image_model_transfer.predict(image_batch)\n","            \n","    return transfer_values\n","\n","\n","def proces_transfer(vid_names, in_dir, labels):\n","    \n","    count = 0\n","    \n","    tam = len(vid_names)\n","    \n","    # Pre-allocate input-batch-array for images.\n","    shape = (_images_per_file,) + img_size_touple + (3,)\n","    \n","    while count<tam:\n","        \n","        video_name = vid_names[count]\n","        \n","        image_batch = np.zeros(shape=shape, dtype=np.float16)\n","    \n","        image_batch = get_frames(in_dir, video_name)\n","        \n","         # Note that we use 16-bit floating-points to save memory.\n","        shape = (_images_per_file, transfer_values_size)\n","        transfer_values = np.zeros(shape=shape, dtype=np.float16)\n","        \n","        transfer_values = \\\n","            image_model_transfer.predict(image_batch)\n","         \n","        labels1 = labels[count]\n","        \n","        aux = np.ones([20,2])\n","        \n","        labelss = labels1*aux\n","        \n","        yield transfer_values, labelss\n","        \n","        count+=1\n","\n","\n","def make_files(n_files):\n","    \n","    gen = proces_transfer(names_training, in_dir, labels_training)\n","\n","    numer = 1\n","\n","    # Read the first chunk to get the column dtypes\n","    chunk = next(gen)\n","\n","    row_count = chunk[0].shape[0]\n","    row_count2 = chunk[1].shape[0]\n","    \n","    with h5py.File('prueba.h5', 'w') as f:\n","    \n","        # Initialize a resizable dataset to hold the output\n","        maxshape = (None,) + chunk[0].shape[1:]\n","        maxshape2 = (None,) + chunk[1].shape[1:]\n","    \n","    \n","        dset = f.create_dataset('data', shape=chunk[0].shape, maxshape=maxshape,\n","                                chunks=chunk[0].shape, dtype=chunk[0].dtype)\n","    \n","        dset2 = f.create_dataset('labels', shape=chunk[1].shape, maxshape=maxshape2,\n","                                 chunks=chunk[1].shape, dtype=chunk[1].dtype)\n","    \n","         # Write the first chunk of rows\n","        dset[:] = chunk[0]\n","        dset2[:] = chunk[1]\n","\n","        for chunk in gen:\n","            \n","            if numer == n_files:\n","            \n","                break\n","\n","            # Resize the dataset to accommodate the next chunk of rows\n","            dset.resize(row_count + chunk[0].shape[0], axis=0)\n","            dset2.resize(row_count2 + chunk[1].shape[0], axis=0)\n","\n","            # Write the next chunk\n","            dset[row_count:] = chunk[0]\n","            dset2[row_count:] = chunk[1]\n","\n","            # Increment the row count\n","            row_count += chunk[0].shape[0]\n","            row_count2 += chunk[1].shape[0]\n","            \n","            print_progress(numer, n_files)\n","        \n","            numer += 1\n","\n","\n","\n","def make_files_test(n_files):\n","    \n","    gen = proces_transfer(names_test, in_dir, labels_test)\n","\n","    numer = 1\n","\n","    # Read the first chunk to get the column dtypes\n","    chunk = next(gen)\n","\n","    row_count = chunk[0].shape[0]\n","    row_count2 = chunk[1].shape[0]\n","    \n","    with h5py.File('pruebavalidation.h5', 'w') as f:\n","    \n","        # Initialize a resizable dataset to hold the output\n","        maxshape = (None,) + chunk[0].shape[1:]\n","        maxshape2 = (None,) + chunk[1].shape[1:]\n","    \n","    \n","        dset = f.create_dataset('data', shape=chunk[0].shape, maxshape=maxshape,\n","                                chunks=chunk[0].shape, dtype=chunk[0].dtype)\n","    \n","        dset2 = f.create_dataset('labels', shape=chunk[1].shape, maxshape=maxshape2,\n","                                 chunks=chunk[1].shape, dtype=chunk[1].dtype)\n","    \n","         # Write the first chunk of rows\n","        dset[:] = chunk[0]\n","        dset2[:] = chunk[1]\n","\n","        for chunk in gen:\n","            \n","            if numer == n_files:\n","            \n","                break\n","\n","            # Resize the dataset to accommodate the next chunk of rows\n","            dset.resize(row_count + chunk[0].shape[0], axis=0)\n","            dset2.resize(row_count2 + chunk[1].shape[0], axis=0)\n","\n","            # Write the next chunk\n","            dset[row_count:] = chunk[0]\n","            dset2[row_count:] = chunk[1]\n","\n","            # Increment the row count\n","            row_count += chunk[0].shape[0]\n","            row_count2 += chunk[1].shape[0]\n","            \n","            print_progress(numer, n_files)\n","        \n","            numer += 1\n","\n","\n","\n","\n","\n","training_set = int(len(names)*0.8)\n","test_set = int(len(names)*0.2)\n","\n","names_training = names[0:training_set]\n","names_test = names[training_set:]\n","\n","labels_training = labels[0:training_set]\n","labels_test = labels[training_set:]\n","\n","\n","make_files(training_set)\n","\n","make_files_test(test_set)\n","\n","def process_alldata_training():\n","    \n","    joint_transfer=[]\n","    frames_num=20\n","    count = 0\n","    \n","    with h5py.File('prueba.h5', 'r') as f:\n","            \n","        X_batch = f['data'][:]\n","        y_batch = f['labels'][:]\n","\n","    for i in range(int(len(X_batch)/frames_num)):\n","        inc = count+frames_num\n","        joint_transfer.append([X_batch[count:inc],y_batch[count]])\n","        count =inc\n","        \n","    data =[]\n","    target=[]\n","    \n","    for i in joint_transfer:\n","        data.append(i[0])\n","        target.append(np.array(i[1]))\n","        \n","    return data, target\n","\n","\n","def process_alldata_test():\n","    \n","    joint_transfer=[]\n","    frames_num=20\n","    count = 0\n","    \n","    with h5py.File('pruebavalidation.h5', 'r') as f:\n","            \n","        X_batch = f['data'][:]\n","        y_batch = f['labels'][:]\n","\n","    for i in range(int(len(X_batch)/frames_num)):\n","        inc = count+frames_num\n","        joint_transfer.append([X_batch[count:inc],y_batch[count]])\n","        count =inc\n","        \n","    data =[]\n","    target=[]\n","    \n","    for i in joint_transfer:\n","        data.append(i[0])\n","        target.append(np.array(i[1]))\n","        \n","    return data, target\n","\n","\n","data, target = process_alldata_training()\n","\n","data_test, target_test = process_alldata_test()\n","\n","chunk_size = 4096\n","n_chunks = 20\n","rnn_size = 512\n","\n","model = Sequential()\n","model.add(LSTM(rnn_size, input_shape=(n_chunks, chunk_size)))\n","model.add(Dense(1024))\n","model.add(Activation('relu'))\n","model.add(Dense(50))\n","model.add(Activation('sigmoid'))\n","model.add(Dense(2))\n","model.add(Activation('softmax'))\n","model.compile(loss='mean_squared_error', optimizer='adam',metrics=['accuracy'])\n","\n","\n","\n","epoch = 200\n","batchS = 500\n","\n","history = model.fit(np.array(data[0:750]), np.array(target[0:750]), epochs=epoch,\n","                    validation_data=(np.array(data[750:]), np.array(target[750:])), \n","                    batch_size=batchS, verbose=2)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"fkovjuAWoc0r","colab_type":"code","colab":{}},"source":["def violence_predict(filename):\n","  vid_name = filename\n","  x = get_transfer_values(in_dir, vid_name)\n","  x = np.expand_dims(x,axis=0)\n","  print(model.predict(x))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8g0dya8oo3uj","colab_type":"text"},"source":["# Violence Prediction"]},{"cell_type":"code","metadata":{"id":"qfu9dso1d2kl","colab_type":"code","colab":{}},"source":["filename = \"\"\n","violence_predict(filename)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QWp-vip1pD0K","colab_type":"text"},"source":["# Fire Preparation\n"]},{"cell_type":"code","metadata":{"id":"DJpmdVKt29Cf","colab_type":"code","colab":{}},"source":["!pip install  imageai --upgrade \n","!wget install https://github.com/OlafenwaMoses/FireNET/releases/download/v1.0/fire-dataset.zip\n","!unzip /content/fire-dataset.zip\n","!wget install https://github.com/OlafenwaMoses/ImageAI/releases/download/essential-v4/pretrained-yolov3.h5\n","!wget install https://github.com/OlafenwaMoses/FireNET/releases/download/v1.0/detection_model-ex-33--loss-4.97.h5"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"fAfhSWup3Rw9","colab_type":"code","colab":{}},"source":[" %tensorflow_version 1.x\n","from imageai.Detection.Custom import CustomObjectDetection, CustomVideoObjectDetection\n","import os\n","\n","execution_path = os.getcwd()\n","\n","total_fire_frames = 0\n","total_non = 0\n","\n","def forFrame(frame_number, output_array, output_count):\n","    if bool(output_count):\n","      global total_fire_frames \n","      total_fire_frames  += 1\n","    else:\n","      global total_non\n","      total_non += 1  \n","\n","\n","def detect_from_video(filename):\n","    detector = CustomVideoObjectDetection()\n","    detector.setModelTypeAsYOLOv3()\n","    detector.setModelPath(detection_model_path=os.path.join(execution_path, \"detection_model-ex-33--loss-4.97.h5\"))\n","    detector.setJsonPath(configuration_json=os.path.join(execution_path, \"/content/drive/My Drive/Emergency project/FireDetection/detection_config.json\"))\n","    detector.loadModel()\n","    \n","    detected_video_path = detector.detectObjectsFromVideo(input_file_path=os.path.join(execution_path, filename), frames_per_second=30,  minimum_percentage_probability=30, log_progress=False, save_detected_video=False, per_frame_function=forFrame )\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7TN5tXLKqxQM","colab_type":"text"},"source":["# Fire Prediction"]},{"cell_type":"code","metadata":{"id":"W4R1yDRgC6Nr","colab_type":"code","outputId":"10718829-9d11-492f-9074-02a5e101357a","executionInfo":{"status":"ok","timestamp":1588372800996,"user_tz":-120,"elapsed":35624,"user":{"displayName":"MOHAMMAD KHALED MOSLEMANY","photoUrl":"","userId":"11179830557597909442"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["total_fire_frames = 0\n","total_non = 0\n","filename = \"/content/11.mp4\"\n","detect_from_video(filename)\n","\n","if (total_fire_frames / (total_fire_frames + total_non)) * 100  >= 15:\n","  print(\"FIRE DETECTED\")\n","else:\n","  print(\"NO FIRE DETECTED\")  \n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["FIRE DETECTED\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Emk-Rmb9rqk_","colab_type":"text"},"source":["# Accidents Preparation"]},{"cell_type":"code","metadata":{"id":"atBRMf0SmqG_","colab_type":"code","outputId":"78668e2b-f443-4e33-be49-58b573a6976d","executionInfo":{"status":"ok","timestamp":1588432516898,"user_tz":-120,"elapsed":4315,"user":{"displayName":"MOHAMMAD KHALED MOSLEMANY","photoUrl":"","userId":"11179830557597909442"}},"colab":{"base_uri":"https://localhost:8080/","height":227}},"source":["!wget https://github.com/saifrais/w210-accident-detection/blob/master/inception_v3/retrained_graph.pb"],"execution_count":0,"outputs":[{"output_type":"stream","text":["--2020-05-02 15:15:13--  https://github.com/saifrais/w210-accident-detection/blob/master/inception_v3/retrained_graph.pb\n","Resolving github.com (github.com)... 140.82.118.3\n","Connecting to github.com (github.com)|140.82.118.3|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘retrained_graph.pb’\n","\n","\rretrained_graph.pb      [<=>                 ]       0  --.-KB/s               \rretrained_graph.pb      [ <=>                ]  65.64K  --.-KB/s    in 0.01s   \n","\n","2020-05-02 15:15:14 (5.62 MB/s) - ‘retrained_graph.pb’ saved [67220]\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"1ld4ZNJOm0Jb","colab_type":"code","colab":{}},"source":["\n","import torch \n","from PIL import Image, ImageFile\n","import numpy as np\n","import torch\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import torch.utils.data as data_utils\n","import torchvision\n","from PIL import Image, ImageFile\n","from torch import nn\n","from torch import optim as optim\n","from torch.autograd import Variable\n","from torch.optim import lr_scheduler\n","from torch.utils.data import DataLoader, Dataset\n","from torch.utils.data.sampler import SubsetRandomSampler\n","from torchvision import datasets, models, transforms\n","from google.colab.patches import cv2_imshow\n","import cv2\n","\n","def accidents_predict(filename):\n","\n","  train_on_gpu = torch.cuda.is_available()\n","  if not train_on_gpu:\n","      print('CUDA is not available.  Training on CPU ...')\n","\n","  else:\n","      print('CUDA is available!  Training on GPU ...')\n","  ImageFile.LOAD_TRUNCATED_IMAGES = True\n","  #!pip install --upgrade wandb\n","\n","\n","\n","\n","  test_transforms = transforms.Compose([transforms.Resize(255),\n","                                        #  transforms.CenterCrop(224),\n","                                        transforms.ToTensor(),\n","                                        ])\n","\n","  model = models.densenet161()\n","\n","\n","  model.classifier = nn.Sequential(nn.Linear(2208, 1000),\n","                                  nn.ReLU(),\n","                                  nn.Dropout(0.2),\n","                                  nn.Linear(1000, 2),\n","                                  nn.LogSoftmax(dim=1))\n","\n","  criterion = nn.NLLLoss()\n","  # Only train the classifier parameters, feature parameters are frozen\n","  optimizer = optim.Adam(model.parameters(), lr=0.001)\n","  scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n","\n","  model = model.cuda()\n","\n","  model.load_state_dict(torch.load(\"/content/drive/My Drive/Emergency project/AccidentsDetection/tensorboardexp.pt\"))\n","  classes = [\"accident\", \"noaccident\"]\n","  # model.load_state_dict(torch.load('tensorboardexp.pt'))\n","  count = 0\n","  counts = 1\n","  videopath = filename\n","\n","  label_arr = []\n","  #img_array = []\n","\n","  vid = cv2.VideoCapture(videopath)\n","  ret = True\n","  while ret:\n","      if ret == True:\n","          ret, frame = vid.read()\n","\n","          try:\n","              img = Image.fromarray(frame)\n","          except ValueError:\n","              break\n","          except AttributeError:\n","              break\n","          img = test_transforms(img)\n","          img = img.unsqueeze(dim=0)\n","          img = img.cuda()\n","          model.eval()\n","          with torch.no_grad():\n","              output = model(img)\n","              _, predicted = torch.max(output, 1)\n","\n","              index = int(predicted.item())\n","              if index == 0:\n","                  count += 1\n","                  if counts == 1:\n","                      counts += 1\n","\n","              labels = classes[index]\n","\n","          # cv2.putText(frame, labels, (10, 100),\n","          #             cv2.FONT_HERSHEY_DUPLEX, 2, (0, 0, 255), 5, cv2.LINE_AA)\n","          \n","          # img_array.append(frame)\n","          label_arr.append(labels)\n","\n","          if cv2.waitKey(1) & 0xFF == ord('q'):\n","              break\n","\n","\n","  \n","  return label_arr"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xLZ2ZZzWr0y1","colab_type":"text"},"source":["# Accidents Predict"]},{"cell_type":"code","metadata":{"id":"UgCNxBFa75Um","colab_type":"code","outputId":"2fa47e43-3e49-43ab-9538-49796959964d","executionInfo":{"status":"ok","timestamp":1588433170358,"user_tz":-120,"elapsed":53971,"user":{"displayName":"MOHAMMAD KHALED MOSLEMANY","photoUrl":"","userId":"11179830557597909442"}},"colab":{"base_uri":"https://localhost:8080/","height":52}},"source":["total_accident_frames = 0\n","total_accident_non = 0\n","\n","filename = \"/content/noacc.mp4\"\n","total_accident = accidents_predict(filename)\n","\n","for i in total_accident:\n","  if i == \"accident\":\n","    total_accident_frames += 1\n","  else:\n","    total_accident_non += 1\n","\n","if (total_accident_frames / (total_accident_frames + total_accident_non)) * 100  >= 15:\n","  print(\"ACCIDENT DETECTED\")\n","else:\n","  print(\"NO ACCIDENT DETECTED\")  \n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["CUDA is available!  Training on GPU ...\n","ACCIDENT DETECTED\n"],"name":"stdout"}]}]}